{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdf2image\n",
    "!pip install pytesseract\n",
    "!pip install ocrmypdf\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d54328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708725d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass.getpass(\"Activeloop Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4291793",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import PyPDF2\n",
    "\n",
    "# Directory path where the research papers are stored\n",
    "directory = 'papers/'\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "# Function to count tokens in text\n",
    "def count_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Download the necessary resource for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Iterate over files in the directory\n",
    "file_list = os.listdir(directory)\n",
    "total_tokens = 0\n",
    "\n",
    "for file_name in file_list:\n",
    "    # Filter files with specific extensions if needed\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            num_tokens = count_tokens(text)\n",
    "            print(f\"File: {file_name}\\tTokens: {num_tokens}\")\n",
    "            total_tokens += num_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {file_path}\\n{str(e)}\")\n",
    "\n",
    "print(f\"Total Tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6a917",
   "metadata": {},
   "source": [
    "That's too many, let's split our text up into chunks so they fit into the prompt limit. I'm going a chunk size of 10,000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"papers\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28712fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=500)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(texts)\n",
    "\n",
    "num_tokens_first_doc = llm.get_num_tokens(texts[0].page_content)\n",
    "\n",
    "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6175bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"laasya\"  # replace with your username from app.activeloop.ai\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{username}/research_papers_chunk_size_10000\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=\"hub://laasya/research_papers_chunk_size_10000\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"mmr\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "def chunk_pdf_by_sentence(pdf_path, chunk_size):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        \n",
    "        chunks = []\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "            print(\"TEXT: \" + text)\n",
    "            print(\"*******\")\n",
    "            \n",
    "            sentences = sent_tokenize(text)\n",
    "            num_sentences = len(sentences)\n",
    "            \n",
    "            for i in range(0, num_sentences, chunk_size):\n",
    "                chunk = '  '.join(sentences[i:i+chunk_size])\n",
    "                chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    " \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    " \n",
    "    # traverse in the string\n",
    "    for ele in s:\n",
    "        str1 += ele\n",
    " \n",
    "    # return string\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def chunk_text_by_sentence(file_path, chunk_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.readlines()\n",
    "\n",
    "    text= listToString(text)\n",
    "    chunks = []\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    for i in range(0, num_sentences, chunk_size):\n",
    "        chunk = '  '.join(sentences[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e1568",
   "metadata": {},
   "source": [
    "Using Map Reduce Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "Understand the following research papers regarding how instructors support student motivation by provifing rationales, \n",
    "relevance to the real world or enthusiasm in a classroom. The question is a transcript of a lecture. \n",
    "Classify every sentence in the question as rationale, relevance, enthusiasm or none. \n",
    "Return the exact verbatim of the question, its classification and why it was classified as such. \n",
    "DO NOT change the words in the question, keep it as is.\n",
    "{context}\n",
    "Question: {question}\n",
    "Relevant text, if any:\"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "\n",
    "Given the question and a classification of the question, return the question and your final classification based on \n",
    "the most occuring classification of this question.\n",
    "\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "Answer:\"\"\"\n",
    "COMBINE_PROMPT = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "chunks = chunk_text_by_sentence(\"lab transcript (1).docx.txt\", 10)  # Calling function to chunk the pdf by sentence\n",
    "\n",
    "\n",
    "qa_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=False, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "\n",
    "qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=retriever)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(\"CHUNK:\" + chunl)\n",
    "    result = qa({\"input_documents\":db,\"query\": chunk}, return_only_outputs=True)\n",
    "    print(result)\n",
    "    print(\"-----------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
